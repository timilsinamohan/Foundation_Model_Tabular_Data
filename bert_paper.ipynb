{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1c9e0-a421-4b7c-ad73-dc98c781c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# from torch.utils.data import Dataset\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from transformers import pipeline\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "   \n",
    "set_seed(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU support) is available and enabled!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA (GPU support) is not available. Falling back to CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure PyTorch uses all available threads\n",
    "torch.set_num_threads(torch.get_num_threads())\n",
    "\n",
    "def data_pre_processing(df):\n",
    "    timestamp_columns = [col for col in df.columns if \"timestamp\" in col]\n",
    "    other_non_imp_column_to_remove = [\"id\", \"general_relapse_class\"]\n",
    "    columns_with_all_nan = df.columns[df.isna().all(axis=0)].tolist()\n",
    "    print(\"shape of the dataframe before dropping columns\", df.shape)\n",
    "    df.drop(timestamp_columns + other_non_imp_column_to_remove + columns_with_all_nan, axis=1, inplace=True)\n",
    "    print(\"shape of the dataframe after dropping columns\", df.shape)\n",
    "    categorical = df.select_dtypes(include=['bool', 'object']).columns.tolist()\n",
    "    numerical = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    print(\"Length of categorical and numerical columns:\", len(categorical), len(numerical))\n",
    "    imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "    imputed_data = imputer.fit_transform(df[numerical])\n",
    "    df_imputed_numerical = pd.DataFrame(imputed_data, columns=numerical, index=df.index)\n",
    "    df[numerical] = df_imputed_numerical\n",
    "    return pd.get_dummies(df, columns=categorical, drop_first=True)\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/Tabular_Machine_Learning_Using_LLM/Data/patient_features_early_stage.csv\")\n",
    "df.rename(columns={'relapse?': 'general_relapse_class'}, inplace=True)\n",
    "y = df[\"general_relapse_class\"].values\n",
    "df_encoded = data_pre_processing(df)\n",
    "X = df_encoded.values\n",
    "df_encoded.head()\n",
    "\n",
    "def row_to_sentence_full(row):\n",
    "    sentence_parts = [\n",
    "        f\"A patient of age {row['age']} started chemotherapy at {row['chemotherapy@t1_start_time']} days\",\n",
    "        f\"underwent radiotherapy for {row['radiotherapy@t1_duration_days']} days\",\n",
    "        f\"and had surgery at {row['surgery@t1_time']} days\",\n",
    "        f\"with a history of smoking {row['nb_cig_packs_year']} cigarette packs per year\",\n",
    "        f\"and {row['nb_cigs_day']} cigarettes per day\",\n",
    "    ]\n",
    "    family_history = []\n",
    "    if row['family_lung_cancer']:\n",
    "        family_history.append(\"lung cancer\")\n",
    "    if row['family_other_cancer']:\n",
    "        family_history.append(\"other types of cancer\")\n",
    "    if family_history:\n",
    "        sentence_parts.append(f\"and a family history that includes: {', '.join(family_history)}\")\n",
    "    else:\n",
    "        sentence_parts.append(\"and no family history of cancer\")\n",
    "    for col in row.index:\n",
    "        if pd.api.types.is_numeric_dtype(row[col]) and not '@' in col and col not in ['age', 'Unnamed: 0']:\n",
    "            sentence_parts.append(f\"{col.replace('_', ' ')} of {row[col]}\")\n",
    "        elif pd.api.types.is_bool_dtype(row[col]) and row[col]:\n",
    "            sentence_parts.append(f\"and has a condition of {col.replace('@', ' at ').replace('_', ' ')}\")\n",
    "    sentence = ', '.join(sentence_parts) + '.'\n",
    "    return sentence\n",
    "\n",
    "\n",
    "texts = df_encoded.apply(row_to_sentence_full, axis=1).tolist()\n",
    "labels = y.tolist()\n",
    "\n",
    "data = {'text': texts,\n",
    "        'label':labels}\n",
    "new_df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11731504-6cab-406d-bd47-6d96954ccb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()\n",
    "X = new_df[\"text\"]\n",
    "y = new_df[\"label\"]\n",
    "\n",
    "relapse_df = new_df[new_df['label'] == 1]\n",
    "non_relapse_df = new_df[new_df['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b9b12-45ca-4572-a4d5-01dc4de950ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For relapse patients\n",
    "relapse_train, relapse_test = train_test_split(relapse_df, test_size=100, random_state=42)\n",
    "relapse_train, relapse_val = train_test_split(relapse_train, test_size=50, random_state=42)\n",
    "\n",
    "# For non-relapse patients\n",
    "non_relapse_train, non_relapse_test = train_test_split(non_relapse_df, test_size=100, random_state=42)\n",
    "non_relapse_train, non_relapse_val = train_test_split(non_relapse_train, test_size=50, random_state=42)\n",
    "\n",
    "train_df = pd.concat([relapse_train, non_relapse_train])\n",
    "test_df = pd.concat([relapse_test, non_relapse_test])\n",
    "val_df = pd.concat([relapse_val, non_relapse_val])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce81388-9eaa-41be-9494-fe5da7caf33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split into train+validation and test sets with stratification\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# # Further split train+validation into train and validation sets with stratification\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)  # 0.25 * 0.8 = 0.2 of the original\n",
    "\n",
    "# # Convert pandas DataFrames back to Hugging Face Datasets\n",
    "# train_dataset = Dataset.from_pandas(pd.concat([X_train, y_train], axis=1))\n",
    "# val_dataset = Dataset.from_pandas(pd.concat([X_val, y_val], axis=1))\n",
    "# test_dataset = Dataset.from_pandas(pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356da2b8-aafc-4729-9d9a-1e2f8cf3b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = ['non relapse','relapse']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "# X_train, X_test = train_test_split(new_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# data = Dataset.from_pandas(new_df)\n",
    "# train_val_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "# train_val_dataset = train_val_test_split['train']\n",
    "# test_dataset = train_val_test_split['test']\n",
    "# train_val_split = train_val_dataset.train_test_split(test_size=0.25, seed=42)  # 0.25 x 0.8 = 0.2\n",
    "# train_dataset = train_val_split['train']\n",
    "# val_dataset = train_val_split['test']\n",
    "\n",
    "\n",
    "# train_test_split = data.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access the training and test sets\n",
    "# train_data = train_test_split['train']\n",
    "# test_data = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e852ca3-5f86-4242-aabd-af0d4119a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dd4e4-087f-4d4a-a9ef-9b871a3ec07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffc318-69df-4a09-baa4-d1e32bd3a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be76910-8048-4576-ba12-fb4f35d8482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# tokenized_text = data.map(preprocess_function, batched = True)\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched = True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched = True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497daf9-deea-49b2-992e-2f5fb9eeb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58bc84-2c84-4c88-af81-b2e5ffdba5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20df16-9f58-4995-9b50-59f549307abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f65cf-b5d5-47ba-bf19-656bb697dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffe5c4-a69d-4e50-bb6e-760f56910434",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"No_Relapse\", 1: \"Relapse\"}\n",
    "label2id = {\"No_Relapse\": 0, \"Relapse\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d11de-c925-4b77-a830-67d13f21a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2,id2label=id2label, label2id=label2id).to(device)\n",
    "\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('EleutherAI/gpt-neo-125M', num_labels=2,\n",
    "#                                                            id2label=id2label, label2id=label2id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aafc36-8431-41bf-954a-10dbd32d11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944bf0b-ef71-444e-8143-55fd541943ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c465360-c84d-44fb-bfc1-17f8ac3a1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, _, _ = trainer.predict(tokenized_test)\n",
    "predictions = np.argmax(logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe860f8-3492-4034-bf74-b3b71f7ffcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df011a-2c34-4a62-b18f-05099ecd4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b86c86-0ab1-4056-8487-17d8dcdf416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy:\", accuracy_score(tokenized_test['label'], predictions))\n",
    "print(\"precision:\", precision_score(tokenized_test['label'], predictions))\n",
    "print(\"recall:\", recall_score(tokenized_test['label'], predictions))\n",
    "print(\"f1:\", f1_score(tokenized_test['label'], predictions))\n",
    "print(\"auc_roc:\", roc_auc_score(tokenized_test['label'], torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()[:, 1]))\n",
    "print(\"auc_pr:\", average_precision_score(tokenized_test['label'], torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f44c7-237b-4c4e-86b3-7ecf6df998dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = pipeline(\"relapse_prediction\", model=\"my_awesome_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundmdl",
   "language": "python",
   "name": "foundmdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
